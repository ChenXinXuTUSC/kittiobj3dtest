{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import core\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TORCH_HOME\"] = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('hub/deeplabv3', source=\"local\", model='deeplabv3_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepLabV3(\n",
      "  (backbone): IntermediateLayerGetter(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): DeepLabHead(\n",
      "    (0): ASPP(\n",
      "      (convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ASPPConv(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): ASPPConv(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (3): ASPPConv(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (4): ASPPPooling(\n",
      "          (0): AdaptiveAvgPool2d(output_size=1)\n",
      "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (aux_classifier): FCNHead(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "FCNHead(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.1, inplace=False)\n",
      "  (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.aux_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output['out'] contains the semantic masks, and output['aux'] contains the auxiliary loss values per-pixel. In inference mode, output['aux'] is not useful. So, output['out'] is of shape (N, 21, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.aux_classifier[-1] = nn.Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.classifier[-1] = nn.Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))\n",
    "stretch = nn.Upsample(scale_factor=(4, 1), mode='bilinear', align_corners=False)\n",
    "squeeze = nn.Conv2d(5, 3, kernel_size=3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredom/programfiles/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:3722: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个 64x512 的随机特征图\n",
    "x = torch.randn(4, 5, 64, 512)  # (batch_size, channels, height, width)\n",
    "\n",
    "x = stretch(x)\n",
    "x = squeeze(x)\n",
    "\n",
    "model.train()\n",
    "x = model(x)\n",
    "out, aux = x[\"out\"], x[\"aux\"]\n",
    "\n",
    "out = F.upsample(out, size=(64, 512))\n",
    "aux = F.upsample(aux, size=(64, 512))\n",
    "\n",
    "y = torch.randint(0, 9, (4, 64, 512))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss1 = criterion(out, y)\n",
    "loss2 = criterion(aux, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3(nn.Module):\n",
    "    def __init__(self, num_cls: int, stretch_shape: tuple, in_channels: int):\n",
    "        super(DeepLabV3, self).__init__()\n",
    "        \n",
    "        self.stretch = nn.Upsample(size=stretch_shape, mode='bilinear', align_corners=True)\n",
    "        self.squeeze = nn.Conv2d(in_channels, 3, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.deeplab = torch.hub.load('hub/deeplabv3', source=\"local\", model='deeplabv3_resnet50', pretrained=True)\n",
    "        self.deeplab.classifier[-1] = nn.Conv2d(256, num_cls, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.deeplab.aux_classifier[-1] = nn.Conv2d(256, num_cls, kernel_size=(1, 1), stride=(1, 1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        original_shape = x.shape[2:]\n",
    "        x = self.stretch(x)\n",
    "        x = self.squeeze(x)\n",
    "        x = self.deeplab(x)\n",
    "        return (\n",
    "            F.upsample(x[\"out\"], size=original_shape, mode=\"bilinear\", align_corners=True),\n",
    "            F.upsample(x[\"aux\"], size=original_shape, mode=\"bilinear\", align_corners=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=core.dataset.KITTISpherical(\n",
    "        \"../data\", \"train\",\n",
    "        core.readconfyaml.read(\"../conf/data.yaml\")\n",
    "    ),\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "model = DeepLabV3(\n",
    "    num_cls=9,\n",
    "    stretch_shape=(256, 512),\n",
    "    in_channels=5\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor([0.125, 1.2, 1.1, 0.5, 1.5, 1, 1, 1.2, 1.2], dtype=torch.float32),\n",
    "    ignore_index=-1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4000120162963867\n",
      "3.1682510375976562\n",
      "3.2154836654663086\n",
      "2.8017578125\n",
      "2.423081159591675\n",
      "2.658344030380249\n",
      "2.3561043739318848\n",
      "1.947843313217163\n",
      "2.5080337524414062\n",
      "2.198582887649536\n",
      "1.610268235206604\n",
      "1.7348902225494385\n",
      "1.4596961736679077\n",
      "1.6063477993011475\n",
      "1.7047138214111328\n",
      "1.2857706546783447\n",
      "1.229272484779358\n",
      "2.444493532180786\n",
      "2.2974212169647217\n",
      "1.560075044631958\n",
      "1.1718169450759888\n",
      "1.6614658832550049\n",
      "1.164381742477417\n",
      "1.9247126579284668\n",
      "1.771458625793457\n",
      "1.1350548267364502\n",
      "1.1128251552581787\n",
      "0.8781954050064087\n",
      "0.7269815802574158\n",
      "1.0978459119796753\n",
      "1.7143548727035522\n",
      "1.290608286857605\n",
      "0.9068481922149658\n",
      "0.9184226989746094\n",
      "1.078495740890503\n",
      "1.1456791162490845\n",
      "0.9442987442016602\n",
      "2.0510640144348145\n",
      "0.8484172821044922\n",
      "1.281785011291504\n",
      "1.5556786060333252\n",
      "0.9312343597412109\n",
      "1.6731020212173462\n",
      "0.7015662789344788\n",
      "0.8422929644584656\n",
      "1.0853056907653809\n",
      "0.6909208297729492\n",
      "0.9691123962402344\n",
      "1.0129384994506836\n",
      "0.5143333673477173\n",
      "0.8398679494857788\n",
      "0.8047423362731934\n",
      "1.535339593887329\n",
      "0.6999015808105469\n",
      "1.71181058883667\n",
      "1.6978538036346436\n",
      "0.7881759405136108\n",
      "1.2046464681625366\n",
      "0.6361749172210693\n",
      "0.7814830541610718\n",
      "0.6951996088027954\n",
      "0.911779522895813\n",
      "1.656019926071167\n",
      "0.6122639179229736\n",
      "0.9782559871673584\n",
      "0.7563692927360535\n",
      "1.2504148483276367\n",
      "0.5615250468254089\n",
      "1.1458685398101807\n",
      "0.7279203534126282\n",
      "0.587358832359314\n",
      "0.7148560285568237\n",
      "1.8284928798675537\n",
      "0.8344916701316833\n",
      "0.8432357907295227\n",
      "0.6594994068145752\n",
      "0.41970735788345337\n",
      "0.999467670917511\n",
      "1.1146150827407837\n",
      "0.5298184156417847\n",
      "0.8131991028785706\n",
      "0.42800837755203247\n",
      "2.138230323791504\n",
      "0.5247206091880798\n",
      "0.9951756596565247\n",
      "1.2179124355316162\n",
      "0.4342988133430481\n",
      "0.8802549839019775\n",
      "1.450790524482727\n",
      "0.5168038010597229\n",
      "1.0354830026626587\n",
      "0.6372308135032654\n",
      "0.823262631893158\n",
      "0.6787428259849548\n",
      "0.9010134935379028\n",
      "1.1135419607162476\n",
      "0.3431033492088318\n",
      "1.9527302980422974\n",
      "0.9235566854476929\n",
      "0.9023348093032837\n",
      "1.1872377395629883\n",
      "1.983924388885498\n",
      "0.8512125611305237\n",
      "1.9252097606658936\n",
      "0.7613003253936768\n",
      "0.8145778179168701\n",
      "0.673745334148407\n",
      "0.5739020705223083\n",
      "1.6648260354995728\n",
      "0.9794394969940186\n",
      "0.7799854278564453\n",
      "1.668306589126587\n",
      "1.6714818477630615\n",
      "0.5511263608932495\n",
      "1.1874865293502808\n",
      "0.9204833507537842\n",
      "1.3879122734069824\n",
      "1.1078274250030518\n",
      "0.6349896192550659\n",
      "0.701738715171814\n",
      "0.5089507102966309\n",
      "0.5924193859100342\n",
      "0.5651402473449707\n",
      "0.7638033032417297\n",
      "0.5225533843040466\n",
      "0.4381735324859619\n",
      "0.4005783200263977\n",
      "0.5627851486206055\n",
      "0.794377863407135\n",
      "0.47355759143829346\n",
      "0.363450288772583\n",
      "1.8245975971221924\n",
      "0.49260762333869934\n",
      "0.5051195621490479\n",
      "0.475540429353714\n",
      "1.7828731536865234\n",
      "0.7354735136032104\n",
      "0.6558350324630737\n",
      "0.3523861765861511\n",
      "1.0035429000854492\n",
      "0.819506049156189\n",
      "0.46971964836120605\n",
      "0.8199780583381653\n",
      "1.1123946905136108\n",
      "0.48124849796295166\n",
      "2.9107613563537598\n",
      "0.6280310153961182\n",
      "2.763211250305176\n",
      "0.5540621280670166\n",
      "2.0484139919281006\n",
      "1.2008832693099976\n",
      "0.4867398142814636\n",
      "0.6554979681968689\n",
      "1.0860055685043335\n",
      "1.3706527948379517\n",
      "1.4468342065811157\n",
      "0.6150597333908081\n",
      "1.108299970626831\n",
      "0.7789337635040283\n",
      "1.5524609088897705\n",
      "0.5258111357688904\n",
      "0.5444930791854858\n",
      "0.6935939788818359\n",
      "0.4339214563369751\n",
      "0.9879851937294006\n",
      "0.617161750793457\n",
      "0.6127132773399353\n",
      "0.4900176525115967\n",
      "0.537051260471344\n",
      "0.5611628293991089\n",
      "0.47399425506591797\n",
      "0.8739533424377441\n",
      "2.3285393714904785\n",
      "0.6164935827255249\n",
      "0.808951199054718\n",
      "1.021619439125061\n",
      "1.0846667289733887\n",
      "0.5221817493438721\n",
      "0.749129056930542\n",
      "0.46979203820228577\n",
      "2.2258782386779785\n",
      "0.5440142750740051\n",
      "1.7297556400299072\n",
      "0.9636139869689941\n",
      "0.5902606248855591\n",
      "0.9774831533432007\n",
      "0.595379114151001\n",
      "0.4562681317329407\n",
      "0.9427351951599121\n",
      "0.6459662318229675\n",
      "0.6977593898773193\n",
      "0.42099064588546753\n",
      "0.4966195225715637\n",
      "0.6885427832603455\n",
      "0.9580039978027344\n",
      "0.568023681640625\n",
      "0.6044348478317261\n",
      "0.6945595741271973\n",
      "0.7348917126655579\n",
      "0.5209308862686157\n",
      "0.34505605697631836\n",
      "0.46932095289230347\n",
      "0.9701896905899048\n",
      "0.25724393129348755\n",
      "0.8692100048065186\n",
      "1.0090038776397705\n",
      "1.819854497909546\n",
      "1.7177517414093018\n",
      "0.9680107831954956\n",
      "1.124467372894287\n",
      "0.3951631784439087\n",
      "0.6605071425437927\n",
      "0.5242435932159424\n",
      "0.7505835294723511\n",
      "0.46768587827682495\n",
      "0.507249116897583\n",
      "0.6957734823226929\n",
      "1.324971318244934\n",
      "0.5383164882659912\n",
      "1.0909497737884521\n",
      "0.4905050992965698\n",
      "1.1807408332824707\n",
      "0.867754340171814\n",
      "0.32938075065612793\n",
      "1.1214377880096436\n",
      "0.3915666937828064\n",
      "0.3408104181289673\n",
      "0.7069191932678223\n",
      "0.40304335951805115\n",
      "0.9203828573226929\n",
      "0.7028699517250061\n",
      "0.4196794629096985\n",
      "1.2286955118179321\n",
      "0.32388293743133545\n",
      "0.9848339557647705\n",
      "0.5923205018043518\n",
      "0.8213204145431519\n",
      "0.8301389813423157\n",
      "0.6920130252838135\n",
      "0.6700596213340759\n",
      "1.1055920124053955\n",
      "0.6558951139450073\n",
      "0.9799079895019531\n",
      "0.684363842010498\n",
      "0.7037455439567566\n",
      "0.6202259063720703\n",
      "1.2438983917236328\n",
      "1.5504271984100342\n",
      "1.533803939819336\n",
      "0.568069577217102\n",
      "0.4848178029060364\n",
      "0.528384268283844\n",
      "0.5635230541229248\n",
      "1.760300874710083\n",
      "0.5316976308822632\n",
      "1.0070829391479492\n",
      "0.3931519687175751\n",
      "1.164650559425354\n",
      "0.40118205547332764\n",
      "0.5120333433151245\n",
      "0.8194618225097656\n",
      "1.8319849967956543\n",
      "0.39130908250808716\n",
      "0.7064422369003296\n",
      "0.7436689138412476\n",
      "0.39962804317474365\n",
      "0.4496253728866577\n",
      "0.6577185392379761\n",
      "0.5031707286834717\n",
      "0.7620037794113159\n",
      "0.7515987157821655\n",
      "1.7334494590759277\n",
      "0.5707802772521973\n",
      "1.8677542209625244\n",
      "1.527512550354004\n",
      "0.39374080300331116\n",
      "0.8953553438186646\n",
      "0.8454763889312744\n",
      "1.1564078330993652\n",
      "0.7578988075256348\n",
      "0.5569108128547668\n",
      "1.2542327642440796\n",
      "0.7106992602348328\n",
      "0.7014724016189575\n",
      "0.8512474894523621\n",
      "0.6009875535964966\n",
      "0.6321566104888916\n",
      "0.7710782289505005\n",
      "0.6626081466674805\n",
      "0.7516571879386902\n",
      "0.5044429302215576\n",
      "0.37169817090034485\n",
      "0.6111059188842773\n",
      "0.41274136304855347\n",
      "2.7274699211120605\n",
      "0.46690165996551514\n",
      "1.45060133934021\n",
      "1.034513235092163\n",
      "0.33986470103263855\n",
      "0.7125374674797058\n",
      "0.4703317880630493\n",
      "0.317292183637619\n",
      "0.3550150394439697\n",
      "0.33421799540519714\n",
      "0.5987406969070435\n",
      "0.5957580804824829\n",
      "1.2525802850723267\n",
      "0.3490433394908905\n",
      "0.2554585039615631\n",
      "0.5474415421485901\n",
      "1.4358699321746826\n",
      "0.4724275469779968\n",
      "0.8517067432403564\n",
      "0.4943331778049469\n",
      "0.38636094331741333\n",
      "0.49850189685821533\n",
      "0.30056414008140564\n",
      "1.3075069189071655\n",
      "0.4465341866016388\n",
      "0.4260138273239136\n",
      "0.5506437420845032\n",
      "0.32438814640045166\n",
      "1.2612560987472534\n",
      "0.36298197507858276\n",
      "0.697597861289978\n",
      "1.1736400127410889\n",
      "0.9446083307266235\n",
      "0.4917256236076355\n",
      "1.577877402305603\n",
      "0.6484628319740295\n",
      "0.7149115800857544\n",
      "1.3856143951416016\n",
      "0.39757877588272095\n",
      "0.7830960750579834\n",
      "0.858330488204956\n",
      "0.8932166695594788\n",
      "1.0958871841430664\n",
      "0.4425160884857178\n",
      "0.5492510795593262\n",
      "0.7447891235351562\n",
      "0.807640016078949\n",
      "0.4749523103237152\n",
      "0.6589914560317993\n",
      "0.5385062098503113\n",
      "0.4457705616950989\n",
      "1.1368236541748047\n",
      "0.4900586009025574\n",
      "2.1181774139404297\n",
      "0.8942577242851257\n",
      "1.8408772945404053\n",
      "1.039194107055664\n",
      "0.5636504292488098\n",
      "0.4483972191810608\n",
      "0.3043653666973114\n",
      "0.5209294557571411\n",
      "0.8809750080108643\n",
      "1.3191256523132324\n",
      "0.40207409858703613\n",
      "1.000287413597107\n",
      "0.2671124041080475\n",
      "1.2858127355575562\n",
      "1.072839617729187\n",
      "0.3336784839630127\n",
      "0.757509708404541\n",
      "0.8355119228363037\n",
      "0.83304762840271\n",
      "0.46777695417404175\n",
      "1.6250178813934326\n",
      "0.7490427494049072\n",
      "0.5551255941390991\n",
      "0.6721908450126648\n",
      "0.5594726800918579\n",
      "0.45781993865966797\n",
      "1.0122733116149902\n",
      "0.9007650017738342\n",
      "0.4969451427459717\n",
      "0.667685329914093\n",
      "0.4377039968967438\n",
      "0.8069019317626953\n",
      "0.46503746509552\n",
      "0.4323524832725525\n",
      "0.9164700508117676\n",
      "0.578325629234314\n",
      "0.48844248056411743\n",
      "0.6447693705558777\n",
      "0.43778836727142334\n",
      "0.3658527135848999\n",
      "0.7962286472320557\n",
      "0.8734128475189209\n",
      "0.45733487606048584\n",
      "0.6553860902786255\n",
      "0.6480511426925659\n",
      "0.7861080765724182\n",
      "0.469721257686615\n",
      "1.6768312454223633\n",
      "0.4581427276134491\n",
      "0.5089172124862671\n",
      "1.9144864082336426\n",
      "1.3981568813323975\n",
      "1.0117223262786865\n",
      "0.43306392431259155\n",
      "0.5905019640922546\n",
      "0.8763378262519836\n",
      "0.473506897687912\n",
      "0.6913262605667114\n",
      "0.5563598275184631\n",
      "0.47941863536834717\n",
      "1.1251051425933838\n",
      "0.5520820617675781\n",
      "0.8677279949188232\n",
      "0.4697689116001129\n",
      "0.5553908348083496\n",
      "0.7797538042068481\n",
      "0.4069860577583313\n",
      "0.9180423617362976\n",
      "1.135101318359375\n",
      "0.22322794795036316\n",
      "1.2264373302459717\n",
      "0.5123210549354553\n",
      "0.43457847833633423\n",
      "0.4227949380874634\n",
      "0.9976038932800293\n",
      "0.9184815883636475\n",
      "1.0673792362213135\n",
      "0.8085401058197021\n",
      "0.5842950940132141\n",
      "1.2311595678329468\n",
      "1.2027251720428467\n",
      "0.6096674203872681\n",
      "0.4316493272781372\n",
      "0.6277762055397034\n",
      "0.6489801406860352\n",
      "0.8532465696334839\n",
      "0.4339824318885803\n",
      "0.7721936106681824\n",
      "1.4575300216674805\n",
      "0.7410971522331238\n",
      "0.24850067496299744\n",
      "0.8909898996353149\n",
      "0.49468713998794556\n",
      "0.4944823980331421\n",
      "0.38817858695983887\n",
      "0.28542089462280273\n",
      "0.35741326212882996\n",
      "0.3898733854293823\n",
      "0.35230177640914917\n",
      "0.994618833065033\n",
      "0.4644524157047272\n",
      "0.8255584239959717\n",
      "0.448557049036026\n",
      "0.6825156807899475\n",
      "0.5067560076713562\n",
      "0.6225996613502502\n",
      "0.46162235736846924\n",
      "0.661514163017273\n",
      "0.4329811930656433\n",
      "0.5460436940193176\n",
      "0.5603164434432983\n",
      "0.27467989921569824\n",
      "1.1351182460784912\n",
      "1.1559627056121826\n",
      "0.5077065825462341\n",
      "0.45562630891799927\n",
      "0.5454591512680054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (fmap, gdth) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      3\u001b[0m         fmap \u001b[38;5;241m=\u001b[39m fmap\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m         gdth \u001b[38;5;241m=\u001b[39m gdth\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/programfiles/miniconda3/envs/torch/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    for batch_idx, (fmap, gdth) in enumerate(train_dataloader):\n",
    "        fmap = fmap.to(device).float()\n",
    "        gdth = gdth.to(device).long()\n",
    "\n",
    "        pred_out, pred_aux = model(fmap)\n",
    "\n",
    "        loss_out = criterion(pred_out, gdth)\n",
    "        loss_aux = criterion(pred_aux, gdth)\n",
    "        loss = loss_out + 0.5 * loss_aux\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss.item())\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
